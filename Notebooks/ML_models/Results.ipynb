{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ML models results overview"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Liberie varie da installare"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inclusione delle librerie utilizzate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "# Change plot output format\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "from pandas import ExcelWriter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Variabili di gestione files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Path of the directory containing .pkl file of the different ML models results, can be changed\n",
    "DATASET_PATH = r\"C:\\Users\\Andre\\OneDrive - Università degli Studi di Parma\\Tirocinio\\ML_models\\ML_model_experiments.pkl\"\n",
    "\n",
    "# True = export summary file in the OUTPUT_PATH\n",
    "to_export = True\n",
    "\n",
    "# Content of the exported document, possible values are 'Rankings' or 'Description',\n",
    "# the default value is 'Description'\n",
    "export_content = \"Rankings\"\n",
    "\n",
    "# Path of the output file report, can be changed\n",
    "OUTPUT_PATH = r\"C:\\Users\\Andre\\OneDrive - Università degli Studi di Parma\\Tirocinio\\ML_models\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Leggo il dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(DATASET_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rimuovo eventuali esperimenti duplicati"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.drop_duplicates(subset=[ 'Accuracy - test',\n",
    "                                 'Precision - test',\n",
    "                                 'Specificity - test',\n",
    "                                 'Recall - test',\n",
    "                                 'F1-score - test',\n",
    "                                 'AUC - test'],\n",
    "                        keep='last',\n",
    "                        inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mostro i risultati ordinati per accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.sort_values(by=['F1-score - test'], ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot model type"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a dataset list in order to export a unique excel file with multiple sheets\n",
    "dataset_list = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Model type\", \"Accuracy - test\", 'Specificity - test', \"Precision - test\",\n",
    "                \"Recall - test\", \"F1-score - test\", \"AUC - test\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d1 = dataset[keep_columns].groupby([\"Model type\"]).mean().sort_values(by=['F1-score - test'], ascending=False)\n",
    "else:\n",
    "    d1 = dataset[keep_columns].groupby([\"Model type\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d1.round(3))\n",
    "d1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 6))\n",
    "sns.barplot(x=\"Model type\", y=\"Accuracy - test\", data=dataset)\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/Accuracy_test.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 6))\n",
    "sns.barplot(x=\"Model type\", y=\"Specificity - test\", data=dataset)\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/Specificity_test.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 6))\n",
    "sns.barplot(x=\"Model type\", y=\"Precision - test\", data=dataset)\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/Precision_test.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 6))\n",
    "sns.barplot(x=\"Model type\", y=\"Recall - test\", data=dataset)\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/Recall_test.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 6))\n",
    "sns.barplot(x=\"Model type\", y=\"F1-score - test\", data=dataset)\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/F1-score_test.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 6))\n",
    "sns.barplot(x=\"Model type\", y=\"AUC - test\", data=dataset)\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/AUC_test.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = pd.melt(dataset, id_vars=['Model type'],\n",
    "                  value_vars=['Accuracy - test', 'Specificity - test', 'Precision - test', 'Recall - test', 'F1-score - test', 'AUC - test'],\n",
    "                  var_name='Metric')\n",
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 6))\n",
    "sns.barplot(x=\"Model type\", y=\"value\", hue=\"Metric\", data=results, orient=\"v\")\n",
    "plt.legend(loc='lower left')\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/Metrics_test.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot dataset train/test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Dataset - train/test\", \"Accuracy - test\", 'Specificity - test', \"Precision - test\",\n",
    "                \"Recall - test\", \"F1-score - test\", \"AUC - test\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d2 = dataset[keep_columns].groupby([\"Dataset - train/test\"]).mean().sort_values(by=['F1-score - test'], ascending=False)\n",
    "else:\n",
    "    d2 = dataset[keep_columns].groupby([\"Dataset - train/test\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d2.round(3))\n",
    "d2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 6))\n",
    "sns.barplot(x=\"Accuracy - test\", y=\"Dataset - train/test\", data=dataset, orient=\"h\")\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/Dataset_accuracy_test.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 6))\n",
    "sns.barplot(x=\"Specificity - test\", y=\"Dataset - train/test\", data=dataset, orient=\"h\")\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/Dataset_specificity_test.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 6))\n",
    "sns.barplot(x=\"Precision - test\", y=\"Dataset - train/test\", data=dataset, orient=\"h\")\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/Dataset_precision_test.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 6))\n",
    "sns.barplot(x=\"Recall - test\", y=\"Dataset - train/test\", data=dataset, orient=\"h\")\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/Dataset_recall_test.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 6))\n",
    "sns.barplot(x=\"AUC - test\", y=\"Dataset - train/test\", data=dataset, orient=\"h\")\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/Dataset_AUC_test.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 6))\n",
    "sns.barplot(x=\"F1-score - test\", y=\"Dataset - train/test\", data=dataset, orient=\"h\")\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/Dataset_F1-score_test.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = pd.melt(dataset, id_vars=['Dataset - train/test'],\n",
    "                  value_vars=['Accuracy - test', 'Specificity - test', 'Precision - test', 'Recall - test', 'F1-score - test', 'AUC - test'],\n",
    "                  var_name='Metric')\n",
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 20))\n",
    "sns.barplot(x=\"value\", y=\"Dataset - train/test\", hue=\"Metric\", data=results, orient=\"h\")\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "# Change y labels in order to have more space for the graph part\n",
    "ylabels = list(dict.fromkeys(results[\"Dataset - train/test\"].to_list()))\n",
    "ylabels_new = [label.replace('filtered_active_bankruptcy_', 'filtered_active_bankruptcy_\\n') for label in ylabels]\n",
    "plt.yticks(np.arange(len(ylabels_new)), ylabels_new)\n",
    "\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/Dataset_metrics_test.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot model type & dataset train/test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Model type\", \"Dataset - train/test\", \"Accuracy - test\", 'Specificity - test',\n",
    "                \"Precision - test\", \"Recall - test\", \"F1-score - test\", \"AUC - test\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d3 = dataset[keep_columns].groupby([\"Model type\", \"Dataset - train/test\"]).mean().sort_values(by=['F1-score - test'], ascending=False)\n",
    "else:\n",
    "    d3 = dataset[keep_columns].groupby([\"Model type\", \"Dataset - train/test\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d3.round(3))\n",
    "d3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot number of components"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Number of features\", \"Accuracy - test\", 'Specificity - test', \"Precision - test\",\n",
    "                \"Recall - test\", \"F1-score - test\", \"AUC - test\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d4 = dataset[keep_columns].groupby([\"Number of features\"]).mean().sort_values(by=['F1-score - test'], ascending=False)\n",
    "else:\n",
    "    d4 = dataset[keep_columns].groupby([\"Number of features\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d4.round(3))\n",
    "d4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = pd.melt(dataset[dataset[\"Dimensionality reduction technique\"] == \"N.A.\"], id_vars=['Number of features'],\n",
    "                  value_vars=['Accuracy - test', 'Specificity - test', 'Precision - test', 'Recall - test', 'F1-score - test', 'AUC - test'],\n",
    "                  var_name='Metric')\n",
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 6))\n",
    "sns.barplot(x=\"Number of features\", y=\"value\", hue=\"Metric\", data=results, orient=\"v\")\n",
    "plt.legend(loc='lower left')\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/Number_of_features_metrics.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot model type & number of components"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Model type\", \"Number of features\", \"Accuracy - test\", 'Specificity - test',\n",
    "                \"Precision - test\", \"Recall - test\", \"F1-score - test\", \"AUC - test\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d5 = dataset[keep_columns].groupby([\"Model type\", \"Number of features\"]).mean().sort_values(by=['F1-score - test'], ascending=False)\n",
    "else:\n",
    "    d5 = dataset[keep_columns].groupby([\"Model type\", \"Number of features\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d5.round(3))\n",
    "d5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot dimensionality reduction technique & number of components"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Dimensionality reduction technique\", \"Number of features\", \"Accuracy - test\", 'Specificity - test',\n",
    "                \"Precision - test\", \"Recall - test\", \"F1-score - test\", \"AUC - test\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d6 = dataset[keep_columns].groupby([\"Dimensionality reduction technique\", \"Number of features\"]).mean().sort_values(by=['F1-score - test'], ascending=False)\n",
    "else:\n",
    "    d6 = dataset[keep_columns].groupby([\"Dimensionality reduction technique\", \"Number of features\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d6.round(3))\n",
    "d6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot dimensionality reduction technique"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Dimensionality reduction technique\", \"Accuracy - test\", 'Specificity - test',\n",
    "                \"Precision - test\", \"Recall - test\", \"F1-score - test\", \"AUC - test\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d7 = dataset[keep_columns].groupby([\"Dimensionality reduction technique\"]).mean().sort_values(by=['F1-score - test'], ascending=False)\n",
    "else:\n",
    "    d7 = dataset[keep_columns].groupby([\"Dimensionality reduction technique\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d7.round(3))\n",
    "d7"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = pd.melt(dataset, id_vars=['Dimensionality reduction technique'],\n",
    "                  value_vars=['Accuracy - test', 'Specificity - test', 'Precision - test', 'Recall - test', 'F1-score - test', 'AUC - test'],\n",
    "                  var_name='Metric')\n",
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 6))\n",
    "sns.barplot(x=\"Dimensionality reduction technique\", y=\"value\", hue=\"Metric\", data=results, orient=\"v\")\n",
    "plt.legend(loc='lower left')\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/Dimensionality_reduction_technique_score_test.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot imbalanced data technique"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Imbalanced data technique\", \"Accuracy - test\", 'Specificity - test',\n",
    "                \"Precision - test\", \"Recall - test\", \"F1-score - test\", \"AUC - test\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d8 = dataset[keep_columns].groupby([\"Imbalanced data technique\"]).mean().sort_values(by=['F1-score - test'], ascending=False)\n",
    "else:\n",
    "    d8 = dataset[keep_columns].groupby([\"Imbalanced data technique\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d8.round(3))\n",
    "d8"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = pd.melt(dataset, id_vars=['Imbalanced data technique'],\n",
    "                  value_vars=['Accuracy - test', 'Specificity - test', 'Precision - test', 'Recall - test', 'F1-score - test', 'AUC - test'],\n",
    "                  var_name='Metric')\n",
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 6))\n",
    "sns.barplot(x=\"Imbalanced data technique\", y=\"value\", hue=\"Metric\", data=results, orient=\"v\")\n",
    "plt.legend(loc='lower left')\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/Imbalanced_data_technique_score_test.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot Imbalanced data technique & Dataset train/test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Imbalanced data technique\", \"Dataset - train/test\", \"Accuracy - test\", 'Specificity - test',\n",
    "                \"Precision - test\", \"Recall - test\", \"F1-score - test\", \"AUC - test\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d9 = dataset[keep_columns].groupby([\"Imbalanced data technique\", \"Dataset - train/test\"]).mean().sort_values(by=['F1-score - test'], ascending=False)\n",
    "else:\n",
    "    d9 = dataset[keep_columns].groupby([\"Imbalanced data technique\", \"Dataset - train/test\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d9.round(3))\n",
    "d9"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot model & validation metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Model type\", \"Accuracy - validation\", 'Specificity - validation', \"Precision - validation\",\n",
    "                \"Recall - validation\", \"F1-score - validation\", \"AUC - validation\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d10 = dataset[keep_columns].groupby([\"Model type\"]).mean().sort_values(by=['F1-score - validation'], ascending=False)\n",
    "else:\n",
    "    d10 = dataset[keep_columns].groupby([\"Model type\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d10.round(3))\n",
    "d10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = pd.melt(dataset, id_vars=['Model type'],\n",
    "                  value_vars=['Accuracy - validation', 'Specificity - validation', 'Precision - validation',\n",
    "                              'Recall - validation', 'F1-score - validation', 'AUC - validation'],\n",
    "                  var_name='Metric')\n",
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 6))\n",
    "sns.barplot(x=\"Model type\", y=\"value\", hue=\"Metric\", data=results, orient=\"v\")\n",
    "plt.legend(loc='lower left')\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/Metrics_validation.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot model & training time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Model type\", \"Training time\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d11 = dataset[keep_columns].groupby([\"Model type\"]).mean().sort_values(by=['Training time'], ascending=False)\n",
    "else:\n",
    "    d11 = dataset[keep_columns].groupby([\"Model type\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d11.round(3))\n",
    "d11"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 6))\n",
    "ax = sns.barplot(x=\"Model type\", y=\"Training time\", data=dataset)\n",
    "ax.set(ylabel='Training time (s)')\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/Training_time.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot train/test split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Train/Test split\", \"Accuracy - test\", 'Specificity - test',\n",
    "                \"Precision - test\", \"Recall - test\", \"F1-score - test\", \"AUC - test\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d12 = dataset[keep_columns].groupby([\"Train/Test split\"]).mean().sort_values(by=['F1-score - test'], ascending=False)\n",
    "else:\n",
    "    d12 = dataset[keep_columns].groupby([\"Train/Test split\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d12.round(3))\n",
    "d12"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = pd.melt(dataset, id_vars=['Train/Test split'],\n",
    "                  value_vars=['Accuracy - test', 'Specificity - test', 'Precision - test', 'Recall - test', 'F1-score - test', 'AUC - test'],\n",
    "                  var_name='Metric')\n",
    "sns.set_theme(style=\"white\")\n",
    "plt.subplots(figsize=(9, 6))\n",
    "sns.barplot(x=\"Train/Test split\", y=\"value\", hue=\"Metric\", data=results, orient=\"v\")\n",
    "plt.legend(loc='lower left')\n",
    "if to_export:\n",
    "    plt.savefig(OUTPUT_PATH + \"/Classification_metrics/Train_test_split.pdf\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Esporto in formato xlsx se richiesto"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function in order to generate a unique excel file with multiple sheets\n",
    "# from multiple pandas datasets\n",
    "def save_xls(list_dfs, xls_path, na_replace):\n",
    "    with ExcelWriter(xls_path, engine=\"xlsxwriter\") as writer:\n",
    "        for n, df in enumerate(list_dfs):\n",
    "            if na_replace:\n",
    "                df.to_excel(writer, 'sheet%s' % n, na_rep=\"N.A.\")\n",
    "            else:\n",
    "                df.to_excel(writer, 'sheet%s' % n)\n",
    "            writer.sheets[\"sheet\"+str(n)].set_column(0, 10, 35)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if to_export:\n",
    "    # Experiments dataset\n",
    "    dataset.to_excel(OUTPUT_PATH + \"/ML_model_experiments.xlsx\", engine='xlsxwriter')\n",
    "    if export_content == \"Rankings\":\n",
    "        na_rep = True\n",
    "    else:\n",
    "        na_rep = False\n",
    "    # Rankings/Description datasets\n",
    "    save_xls(dataset_list, OUTPUT_PATH + \"/\" + export_content.lower() + \".xlsx\", na_replace=na_rep)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}