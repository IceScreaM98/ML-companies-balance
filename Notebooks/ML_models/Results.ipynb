{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ML models results overview"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Liberie varie da installare"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inclusione delle librerie utilizzate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import ExcelWriter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Variabili di gestione files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Path of the directory containing .pkl file of the different ML models results, can be changed\n",
    "DATASET_PATH = r\"C:\\Users\\Andre\\OneDrive - Università degli Studi di Parma\\Tirocinio\\ML_models\\ML_model_experiments.pkl\"\n",
    "\n",
    "# True = export summary file in the OUTPUT_PATH\n",
    "to_export = True\n",
    "\n",
    "# Content of the exported document, possible values are 'Rankings' or 'Description',\n",
    "# the default value is 'Description'\n",
    "export_content = \"Description\"\n",
    "\n",
    "# Path of the output file report, can be changed\n",
    "OUTPUT_PATH = r\"C:\\Users\\Andre\\OneDrive - Università degli Studi di Parma\\Tirocinio\\ML_models\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Leggo il dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(DATASET_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rimuovo eventuali esperimenti duplicati"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.drop_duplicates(subset=[ 'Accuracy - test',\n",
    "                                 'Precision - test',\n",
    "                                 'Recall - test',\n",
    "                                 'F1-score - test',\n",
    "                                 'AUC - test'],\n",
    "                        keep='last',\n",
    "                        inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mostro i risultati ordinati per accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.sort_values(by=['F1-score - validation'], ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot model type"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a dataset list in order to export a unique excel file with multiple sheets\n",
    "dataset_list = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Model type\", \"Accuracy - test\", \"Precision - test\",\n",
    "                \"Recall - test\", \"F1-score - test\", \"AUC - test\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d1 = dataset[keep_columns].groupby([\"Model type\"]).mean().sort_values(by=['F1-score - test'], ascending=False)\n",
    "else:\n",
    "    d1 = dataset[keep_columns].groupby([\"Model type\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d1)\n",
    "d1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot dataset train/test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Dataset - train/test\", \"Accuracy - test\", \"Precision - test\",\n",
    "                \"Recall - test\", \"F1-score - test\", \"AUC - test\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d2 = dataset[keep_columns].groupby([\"Dataset - train/test\"]).mean().sort_values(by=['F1-score - test'], ascending=False)\n",
    "else:\n",
    "    d2 = dataset[keep_columns].groupby([\"Dataset - train/test\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d2)\n",
    "d2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot model type & dataset train/test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Model type\", \"Dataset - train/test\", \"Accuracy - test\",\n",
    "                \"Precision - test\", \"Recall - test\", \"F1-score - test\",\n",
    "                \"AUC - test\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d3 = dataset[keep_columns].groupby([\"Model type\", \"Dataset - train/test\"]).mean().sort_values(by=['F1-score - test'], ascending=False)\n",
    "else:\n",
    "    d3 = dataset[keep_columns].groupby([\"Model type\", \"Dataset - train/test\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d3)\n",
    "d3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot number of components"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Number of components\", \"Accuracy - test\", \"Precision - test\",\n",
    "                \"Recall - test\", \"F1-score - test\", \"AUC - test\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d4 = dataset[keep_columns].groupby([\"Number of components\"]).mean().sort_values(by=['F1-score - test'], ascending=False)\n",
    "else:\n",
    "    d4 = dataset[keep_columns].groupby([\"Number of components\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d4)\n",
    "d4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot model type & number of components"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Model type\", \"Number of components\", \"Accuracy - test\", \"Precision - test\",\n",
    "                \"Recall - test\", \"F1-score - test\", \"AUC - test\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d5 = dataset[keep_columns].groupby([\"Model type\", \"Number of components\"]).mean().sort_values(by=['F1-score - test'], ascending=False)\n",
    "else:\n",
    "    d5 = dataset[keep_columns].groupby([\"Model type\", \"Number of components\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d5)\n",
    "d5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot dimensionality reduction technique"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Dimensionality reduction technique\", \"Accuracy - test\",\n",
    "                \"Precision - test\", \"Recall - test\", \"F1-score - test\", \"AUC - test\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d6 = dataset[keep_columns].groupby([\"Dimensionality reduction technique\"]).mean().sort_values(by=['F1-score - test'], ascending=False)\n",
    "else:\n",
    "    d6 = dataset[keep_columns].groupby([\"Dimensionality reduction technique\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d6)\n",
    "d6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot imbalanced data technique"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Imbalanced data technique\", \"Accuracy - test\",\n",
    "                \"Precision - test\", \"Recall - test\", \"F1-score - test\", \"AUC - test\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d7 = dataset[keep_columns].groupby([\"Imbalanced data technique\"]).mean().sort_values(by=['F1-score - test'], ascending=False)\n",
    "else:\n",
    "    d7 = dataset[keep_columns].groupby([\"Imbalanced data technique\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d7)\n",
    "d7"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot Imbalanced data technique & Dataset train/test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Imbalanced data technique\", \"Dataset - train/test\", \"Accuracy - test\",\n",
    "                \"Precision - test\", \"Recall - test\", \"F1-score - test\", \"AUC - test\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d8 = dataset[keep_columns].groupby([\"Imbalanced data technique\", \"Dataset - train/test\"]).mean().sort_values(by=['F1-score - test'], ascending=False)\n",
    "else:\n",
    "    d8 = dataset[keep_columns].groupby([\"Imbalanced data technique\", \"Dataset - train/test\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d8)\n",
    "d8"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot model & validation metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_columns = [\"Model type\", \"Accuracy - validation\", \"Precision - validation\",\n",
    "                \"Recall - validation\", \"F1-score - validation\", \"AUC - validation\"]\n",
    "if export_content == \"Rankings\":\n",
    "    d9 = dataset[keep_columns].groupby([\"Model type\"]).mean().sort_values(by=['F1-score - validation'], ascending=False)\n",
    "else:\n",
    "    d9 = dataset[keep_columns].groupby([\"Model type\"]).agg([np.mean, np.std])\n",
    "dataset_list.append(d9)\n",
    "d9"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Esporto in formato xlsx se richiesto"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function in order to generate a unique excel file with multiple sheets\n",
    "# from multiple pandas datasets\n",
    "def save_xls(list_dfs, xls_path, na_rep):\n",
    "    with ExcelWriter(xls_path, engine=\"xlsxwriter\") as writer:\n",
    "        for n, df in enumerate(list_dfs):\n",
    "            if na_rep:\n",
    "                df.to_excel(writer, 'sheet%s' % n, na_rep=\"N.A.\")\n",
    "            else:\n",
    "                df.to_excel(writer, 'sheet%s' % n)\n",
    "            writer.sheets[\"sheet\"+str(n)].set_column(0, 10, 35)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if to_export:\n",
    "    # Experiments dataset\n",
    "    dataset.to_excel(OUTPUT_PATH + \"/ML_model_experiments.xlsx\", engine='xlsxwriter')\n",
    "    if export_content == \"Rankings\":\n",
    "        na_rep = True\n",
    "    else:\n",
    "        na_rep = False\n",
    "    # Rankings/Description datasets\n",
    "    save_xls(dataset_list, OUTPUT_PATH + \"/\" + export_content.lower() + \".xlsx\", na_rep=na_rep)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}