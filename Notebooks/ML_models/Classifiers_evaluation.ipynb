{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation of different supervised classifiers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Liberie varie da installare"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install sklearn\n",
    "#!pip install seaborn\n",
    "#!pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inclusione delle librerie utilizzate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Variabili di gestione files, parametri del modello e della fase di training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Path of the dataset in .pkl format, can be changed\n",
    "PATH_DATASET = r\"C:\\Users\\Andre\\OneDrive - Università degli Studi di Parma\\Tirocinio\\Dataset_output\\filtered_active_bankruptcy_small.pkl\"\n",
    "\n",
    "# True = Standardize data, can be changed\n",
    "to_standardize = True\n",
    "\n",
    "# True = Transform the dataset into an equivalent one with less features, can be changed\n",
    "reduce_feature = False\n",
    "\n",
    "# Specify the number of features we want after applying the dimensionality reduction of the dataset, can be changed\n",
    "number_feature = 15\n",
    "\n",
    "# True = Oversample/Undersample the least/most populated class (Bankruptcy), can be changed\n",
    "avoid_imbalanced_training = True\n",
    "\n",
    "# Oversample or Undersample, can be changed.\n",
    "# It only affects the notebook if avoid_imbalanced_training is True\n",
    "imbalanced_data_technique = \"Oversample\"\n",
    "\n",
    "# True = Also use non financial indexes features like the legal form or the size of the company\n",
    "additional_features = True\n",
    "\n",
    "# A value between [0, 1], it represent the percentage of records not used during training time, can be changed\n",
    "train_test_split_amount = 0.25\n",
    "\n",
    "# Select a random state value in order to control the randomness effect, can be changed\n",
    "rnd_state = 25\n",
    "\n",
    "# Specify the number of cuncurrent jobs in order to speed up certain traning phases.\n",
    "# Specify -1 in order to use all the job available, the default one is 1, can be changed\n",
    "n_jobs = -1\n",
    "\n",
    "# True = Export the current experiment inside the dataframe that collects all of them, can be changed\n",
    "to_export = False\n",
    "\n",
    "# Path of the dataset in .pkl format to store all the experiments, can be changed and if not present it will be created\n",
    "OUTPUT_PATH = r\"C:\\Users\\Andre\\OneDrive - Università degli Studi di Parma\\Tirocinio\\ML_models\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Controllo se esiste il dataset di output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Only if we want to export\n",
    "if to_export:\n",
    "    if exists(OUTPUT_PATH + \"/ML_model_experiments.pkl\"):\n",
    "        # If the dataset exists, read it\n",
    "        output_dataset = pd.read_pickle(OUTPUT_PATH + \"/ML_model_experiments.pkl\")\n",
    "        print(\"Dataset di output trovato\")\n",
    "    else:\n",
    "        # Otherwise create it\n",
    "        output_dataset = pd.DataFrame()\n",
    "        print(\"Dataset di output non trovato, creo un nuovo dataset\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lettura del dataset di input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(PATH_DATASET)\n",
    "print(\"Il dataset da utilizzare ha\", dataset.shape[0], \"record e\", dataset.shape[1], \"colonne\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suddivisione del dataset in X e Y, dove X sono le features in ingresso (indicatori finanziari) e Y è la risposta in output (attivo/bancarotta)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_features_names = ['PN/Totale Debiti',\n",
    "                    'Deb. Prev + Trib/Attivo',\n",
    "                    'Tempo medio riscossione (TMR)',\n",
    "                    'Tempo medio di pagamento (TMP)',\n",
    "                    'PFN/EBITDA',\n",
    "                    'PFN/PN',\n",
    "                    'Gearing',\n",
    "                    'ROS',\n",
    "                    'Working capital/net sales',\n",
    "                    'Cash/Current Liabilities',\n",
    "                    'Accounts receivable/inventory',\n",
    "                    'EBIT/interest expenses',\n",
    "                    'Att.Br/Attivo',\n",
    "                    'Ricavi/Attivo',\n",
    "                    'EBITDA/Totale Debiti']\n",
    "\n",
    "if additional_features:\n",
    "    X_features_names.append('Legal Form')\n",
    "    X_features_names.append('Number of employees')\n",
    "    #X_features_names.append('Company Size')\n",
    "\n",
    "Y_feature_name = 'Legal Status'\n",
    "\n",
    "X_dataset = dataset[X_features_names].copy()\n",
    "\n",
    "if additional_features:\n",
    "    # One hot encoding\n",
    "    X_dataset = X_dataset.join(pd.get_dummies(dataset['Legal Form']))\n",
    "    X_dataset.drop('Legal Form', axis = 1, inplace=True)\n",
    "\n",
    "    #X_dataset = X_dataset.join(pd.get_dummies(dataset['Company Size']))\n",
    "    #X_dataset.drop('Company Size', axis = 1, inplace=True)\n",
    "\n",
    "    # Save the new feature names\n",
    "    X_features_names = X_dataset.columns.to_list()\n",
    "\n",
    "Y_dataset = dataset[Y_feature_name].copy()\n",
    "Y_dataset.replace({\"Active\": 0, \"Bankruptcy\": 1}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Esempio di record"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.concat([X_dataset.head(1), Y_dataset.head(1)], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Uso la tecnica di random oversampling o undersampling per evitare un allenamento di un modello con classi sbilanciate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if avoid_imbalanced_training:\n",
    "    # Oversample\n",
    "    if imbalanced_data_technique == \"Oversample\":\n",
    "        sm = SMOTE(random_state=rnd_state, n_jobs=n_jobs)\n",
    "        X_dataset, Y_dataset = sm.fit_resample(X_dataset, Y_dataset)\n",
    "    # Undersample\n",
    "    elif imbalanced_data_technique == \"Undersample\":\n",
    "        undersample = RandomUnderSampler(sampling_strategy='majority', random_state=rnd_state)\n",
    "        X_dataset, Y_dataset = undersample.fit_resample(X_dataset, Y_dataset)\n",
    "    else:\n",
    "        print(\"Error: wrong variable value about imbalanced data\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Standardizzo i dati contenuti in X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if to_standardize:\n",
    "    scaler = StandardScaler()\n",
    "    X_dataset = scaler.fit_transform(X_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Applico la SVD del dataset con PCA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if reduce_feature:\n",
    "    pca = PCA(n_components=number_feature)\n",
    "    X_dataset = pca.fit_transform(X_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Divido i 2 dataset in train e test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_dataset,\n",
    "                                                    Y_dataset,\n",
    "                                                    stratify=Y_dataset,\n",
    "                                                    test_size=train_test_split_amount,\n",
    "                                                    random_state=rnd_state)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Confronto vari modelli supervisionati"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyperparameters of each model\n",
    "log_reg_params = [{\"C\":0.01}, {\"C\":0.1}, {\"C\":1}, {\"C\":10}]\n",
    "dec_tree_params = [{\"criterion\": \"gini\"}, {\"criterion\": \"entropy\"}]\n",
    "rand_for_params = [{\"criterion\": \"gini\", \"n_jobs\": n_jobs}, {\"criterion\": \"entropy\", \"n_jobs\": n_jobs}]\n",
    "kneighbors_params = [{\"n_neighbors\":3, \"n_jobs\": n_jobs}, {\"n_neighbors\":5, \"n_jobs\": n_jobs}]\n",
    "naive_bayes_params = [{}]\n",
    "svc_params = [{\"C\":0.01}, {\"C\":0.1}, {\"C\":1}, {\"C\":10}]\n",
    "\n",
    "# List of each model with its hyperparameters\n",
    "supervised_models = [\n",
    "    [\"Logistic Regression\", LogisticRegression, log_reg_params],\n",
    "    [\"Decision Tree\", DecisionTreeClassifier, dec_tree_params],\n",
    "    [\"Random Forest\", RandomForestClassifier, rand_for_params],\n",
    "   # [\"K Neighbors\", KNeighborsClassifier, kneighbors_params],\n",
    "   # [\"Naive Bayes\", GaussianNB, naive_bayes_params],\n",
    "   # [\"Support Vector Machine\", SVC, svc_params]\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores = []\n",
    "for model_name, Model, params_list in supervised_models:\n",
    "    for params in params_list:\n",
    "        # Define model\n",
    "        if model_name in [\"K Neighbors\", \"Naive Bayes\"]:\n",
    "            model = Model(**params)\n",
    "        else:\n",
    "            model = Model(**params, random_state=rnd_state)\n",
    "        # Train the model\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        Y_predicted = model.predict(X_test)\n",
    "\n",
    "        # Compute different metrics\n",
    "        accuracy = accuracy_score(Y_test, Y_predicted)\n",
    "        precision = precision_score(Y_test, Y_predicted)\n",
    "        recall = recall_score(Y_test, Y_predicted)\n",
    "        f1 = f1_score(Y_test, Y_predicted)\n",
    "\n",
    "        Y_probabilities = model.predict_proba(X_test)\n",
    "        preds = Y_probabilities[:,1]\n",
    "        fpr, tpr, threshold = metrics.roc_curve(Y_test, preds)\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "        if to_export:\n",
    "            # Create a new record\n",
    "            new_record = pd.DataFrame({\"Date\": [date.today()],\n",
    "                                       \"Model type\": [model_name],\n",
    "                                       \"Model parameters\": [model.get_params()],\n",
    "                                       \"Data source\": [PATH_DATASET.split(\"\\\\\")[-1]],\n",
    "                                       \"Features\": [X_features_names],\n",
    "                                       \"Standardized\": [to_standardize],\n",
    "                                       \"Imbalanced data corrections\": [avoid_imbalanced_training],\n",
    "                                       \"Imbalanced data technique\": [imbalanced_data_technique],\n",
    "                                       \"Number of active companies used\": [Y_dataset.value_counts()[0]],\n",
    "                                       \"Number of bankruptcy companies used\": [Y_dataset.value_counts()[1]],\n",
    "                                       \"Train/Test split\": [train_test_split_amount],\n",
    "                                       \"Random state\": [rnd_state],\n",
    "                                       \"Accuracy\": [accuracy],\n",
    "                                       \"Precision\": [precision],\n",
    "                                       \"Recall\": [recall],\n",
    "                                       \"F1-score\": [f1],\n",
    "                                       \"AUC\": [roc_auc]})\n",
    "            # Append the new record\n",
    "            output_dataset = pd.concat([output_dataset, new_record], ignore_index=True, axis=0)\n",
    "\n",
    "        # Friendly print to keep track of the evaluation stage\n",
    "        print(model_name, params, accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Esporto i nuovi esperimenti"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if to_export:\n",
    "    # Export the dataset\n",
    "    output_dataset.to_pickle(OUTPUT_PATH + \"/ML_model_experiments.pkl\")\n",
    "    print(\"Esprimenti aggiunti al dataset\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}