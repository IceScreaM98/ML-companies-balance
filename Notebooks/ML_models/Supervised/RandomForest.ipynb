{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondo prototipo di modello ML: random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liberie varie da installare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install sklearn\n",
    "#!pip install seaborn\n",
    "#!pip install imblearn\n",
    "#!pip install shap -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inclusione delle librerie utilizzate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "from datetime import date\n",
    "import time\n",
    "import re\n",
    "import glob\n",
    "import shap\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, SparsePCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variabili di gestione files, parametri del modello e della fase di training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## INPUT DATASET ###########################################\n",
    "# Path of the dataset in .pkl format, can be changed\n",
    "PATH_DATASET = r\"C:\\Users\\Andre\\OneDrive - Università degli Studi di Parma\\Tirocinio\\Dataset_output\\filtered_active_bankruptcy_raw_full_2_CE.pkl\"\n",
    "##################################################################################################\n",
    "\n",
    "######################################## DATA STANDARDIZATION ####################################\n",
    "# True = Standardize data, can be changed\n",
    "to_standardize = True\n",
    "##################################################################################################\n",
    "\n",
    "######################################## DATA CLASSES BALANCE ####################################\n",
    "# True = Oversample the least populated class (Bankruptcy), can be changed\n",
    "avoid_imbalanced_training = False\n",
    "\n",
    "# Oversample/Undersample the least/most populated class (Bankruptcy), possible values: 'Oversample'\n",
    "# or 'Undersample'\n",
    "# It only affects the notebook if avoid_imbalanced_training is True\n",
    "imbalanced_data_technique = \"Oversample\"\n",
    "##################################################################################################\n",
    "\n",
    "######################################## DIMENSIONALITY REDUCTION ################################\n",
    "# True = use a dimensionality reduction method, can be changed\n",
    "dimensionality_reduction = False\n",
    "\n",
    "# Applies dimensionality reduction method, possible values: 'PCA', 'LDA', 'Incremental PCA',\n",
    "# 'Sparse PCA'\n",
    "# It only affects the notebook if dimensionality_reduction is True\n",
    "dimensionality_reduction_method = \"PCA\"\n",
    "\n",
    "# Number of desired components after applying a dimensionality reduction method\n",
    "# N.B. n_components must be < number of input features and LDA only supports 1 components in\n",
    "# our case\n",
    "n_components = 50\n",
    "##################################################################################################\n",
    "\n",
    "######################################## EXTRA DATASET PARAMETERS ################################\n",
    "# True = Also use non financial indexes features like the legal form or the size of the company\n",
    "additional_features = False\n",
    "##################################################################################################\n",
    "\n",
    "######################################## TRAIN-TEST SPLIT ########################################\n",
    "# A value between [0, 1], it represent the percentage of records not used during training time,\n",
    "# can be changed\n",
    "train_test_split_amount = 0.25\n",
    "##################################################################################################\n",
    "\n",
    "######################################## EXTRA MODEL PARAMETERS ##################################\n",
    "# Select a random state value in order to control the randomness effect, can be changed\n",
    "rnd_state = 25\n",
    "\n",
    "# Specify the number of cuncurrent jobs in order to speed up certain traning phases.\n",
    "# Specify -1 in order to use all the job available, the default one is 1, can be changed\n",
    "n_jobs = -1\n",
    "\n",
    "# Select the number of tree used by the random forest\n",
    "# It should be something between [10, 200], default value is 100\n",
    "n_tree = 100\n",
    "##################################################################################################\n",
    "\n",
    "######################################## SHAP VALUES PARAMETERS ##################################\n",
    "# True = Print shap values plot\n",
    "shap_plot = False\n",
    "##################################################################################################\n",
    "\n",
    "######################################## OUTPUT EXPERIMENT #######################################\n",
    "# True = Export the current experiment inside the dataframe that collects all of them, can be changed\n",
    "to_export = True\n",
    "\n",
    "# Path of the dataset in .pkl format to store all the experiments, can be changed and\n",
    "# if not present it will be created\n",
    "OUTPUT_PATH = r\"C:\\Users\\Andre\\OneDrive - Università degli Studi di Parma\\Tirocinio\\ML_models\"\n",
    "\n",
    "# True = Export the trained model on an external file, can be changed\n",
    "to_export_model = False\n",
    "\n",
    "# Path of the exported model, can be changed\n",
    "OUTPUT_PATH_MODEL = \"\"\n",
    "\n",
    "# True = Export the current classification metrics for each region if available, can be changed\n",
    "to_export_region_metrics = False\n",
    "\n",
    "# Path of the exported regional metrics dataframe as xlsx file, can be changed\n",
    "OUTPUT_REGION_PATH = \"\"\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lettura del dataset di input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(PATH_DATASET)\n",
    "print(\"Il dataset da utilizzare ha\", dataset.shape[0], \"record e\", dataset.shape[1], \"colonne\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suddivisione del dataset in X e Y, dove X sono le features in ingresso e Y è la risposta in output (attivo/bancarotta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove descriptive columns\n",
    "removed_columns = [\n",
    "    \"Ragione sociale\",\n",
    "    \"Province\",\n",
    "    \"Accounting closing date\",\n",
    "    \"Legal Status\",\n",
    "    \"Tax code number\",\n",
    "    \"CCIAA number\"\n",
    "]\n",
    "\n",
    "# List to store region names\n",
    "region_names = []\n",
    "\n",
    "\n",
    "\n",
    "if not additional_features:\n",
    "    removed_columns.append(\"Legal Form\")\n",
    "    removed_columns.append(\"Company Size\")\n",
    "    removed_columns.append(\"Number of employees\")\n",
    "    removed_columns.append(\"Regione\")\n",
    "\n",
    "# X dataset\n",
    "X_dataset = dataset.copy()\n",
    "X_dataset.drop(removed_columns, axis=1, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Columns to be standardized\n",
    "to_be_std_columns = list(X_dataset.columns)\n",
    "\n",
    "# Manage additional features\n",
    "if additional_features:\n",
    "    # One hot encoding\n",
    "    if \"Legal Form\" in X_dataset.columns:\n",
    "        X_dataset = X_dataset.join(pd.get_dummies(dataset['Legal Form']))\n",
    "        X_dataset.drop('Legal Form', axis=1, inplace=True)\n",
    "        to_be_std_columns.remove('Legal Form')\n",
    "\n",
    "\n",
    "    if \"Company Size\" in X_dataset.columns:\n",
    "        X_dataset = X_dataset.join(pd.get_dummies(dataset['Company Size']))\n",
    "        X_dataset.drop('Company Size', axis=1, inplace=True)\n",
    "        to_be_std_columns.remove('Company Size')\n",
    "\n",
    "    if \"Regione\" in X_dataset.columns:\n",
    "        region_names = list(dataset['Regione'].unique())\n",
    "        X_dataset = X_dataset.join(pd.get_dummies(dataset['Regione']))\n",
    "        X_dataset.drop('Regione', axis=1, inplace=True)\n",
    "        to_be_std_columns.remove('Regione')\n",
    "\n",
    "\n",
    "# Save the new feature names\n",
    "X_features_names = X_dataset.columns.to_list()\n",
    "\n",
    "# Y dataset\n",
    "Y_dataset = dataset['Legal Status'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stampo i primi record dei 2 nuovi dataset per chiarezza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codifico la variabile di risposta (Active/Bankruptcy) in (0/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dataset.replace({\"Active\": 0, \"Bankruptcy\": 1}, inplace=True)\n",
    "Y_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controllo il numero di record per ciascuna classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dataset.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uso la tecnica di random oversampling o undersampling per evitare un allenamento di un modello con classi sbilanciate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if avoid_imbalanced_training:\n",
    "    # Oversample (provare anche per undersampling)\n",
    "    if imbalanced_data_technique == \"Oversample\":\n",
    "        sm = SMOTE(random_state=rnd_state, n_jobs=n_jobs)\n",
    "        X_dataset, Y_dataset = sm.fit_resample(X_dataset, Y_dataset)\n",
    "    # Undersample\n",
    "    elif imbalanced_data_technique == \"Undersample\":\n",
    "        undersample = RandomUnderSampler(sampling_strategy='majority', random_state=rnd_state)\n",
    "        X_dataset, Y_dataset = undersample.fit_resample(X_dataset, Y_dataset)\n",
    "    else:\n",
    "        print(\"Error: wrong variable value about imbalanced data\")\n",
    "Y_dataset.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applico algoritmo di riduzione della dimensionalità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dimensionality_reduction:\n",
    "    if dimensionality_reduction_method == \"PCA\":\n",
    "        dimensionality_reduction_model = PCA(n_components=n_components)\n",
    "        X_dataset = dimensionality_reduction_model.fit_transform(X_dataset)\n",
    "    elif dimensionality_reduction_method == \"Incremental PCA\":\n",
    "        dimensionality_reduction_model = IncrementalPCA(n_components=n_components)\n",
    "        X_dataset = dimensionality_reduction_model.fit_transform(X_dataset)\n",
    "    elif dimensionality_reduction_method == \"Sparse PCA\":\n",
    "        dimensionality_reduction_model = SparsePCA(n_components=n_components)\n",
    "        X_dataset = dimensionality_reduction_model.fit_transform(X_dataset)\n",
    "    elif dimensionality_reduction_method == \"LDA\":\n",
    "        n_components = 1\n",
    "        dimensionality_reduction_model = LinearDiscriminantAnalysis(n_components=n_components)\n",
    "        X_dataset = dimensionality_reduction_model.fit_transform(X_dataset, Y_dataset)\n",
    "    # Need to change feature names, because they're lost in the new dataset\n",
    "    X_features_names = [\"Component \" + str(i) for i in range(0, n_components)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizzo i dati contenuti in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if to_standardize:\n",
    "    # Standardize only the features that are not one hot encoded\n",
    "    scaler = ColumnTransformer([\n",
    "        ('STD', StandardScaler(), to_be_std_columns)\n",
    "    ], remainder='passthrough')\n",
    "    X_dataset = pd.DataFrame(scaler.fit_transform(X_dataset))\n",
    "    X_dataset.columns = X_features_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esporto il modello che normalizza i dati su un file esterno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_number = len(glob.glob1(OUTPUT_PATH_MODEL,\"random_forest*.sav\"))\n",
    "scaler_number = len(glob.glob1(OUTPUT_PATH_MODEL,\"standard_scaler*.sav\"))\n",
    "if to_export_model:\n",
    "    # Create the model filename\n",
    "    scaler_filename = \"\"\n",
    "    # Model type\n",
    "    scaler_filename += \"standard_scaler\"\n",
    "    # Enumerate model of the same type\n",
    "    scaler_filename += \"_\" + str(max(model_number, scaler_number))\n",
    "    # File extension\n",
    "    scaler_filename += \".sav\"\n",
    "    # Export the model\n",
    "    joblib.dump(scaler, OUTPUT_PATH_MODEL + \"/\" + scaler_filename)\n",
    "    print(\"Scaler esportato in:\", OUTPUT_PATH_MODEL + \"/\" + scaler_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divido i 2 dataset in train e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_dataset,\n",
    "                                                    Y_dataset,\n",
    "                                                    stratify=Y_dataset,\n",
    "                                                    test_size=train_test_split_amount,\n",
    "                                                    random_state=rnd_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo primo prototipo di random forest e lo alleno su una porzione dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier = RandomForestClassifier(random_state=rnd_state, n_jobs=n_jobs, n_estimators=n_tree)\n",
    "# Add feature names as attribute in order to retrieve them when exported\n",
    "random_forest_classifier.feature_names = X_features_names\n",
    "start = time.time()\n",
    "random_forest_classifier.fit(X_train, Y_train)\n",
    "stop = time.time()\n",
    "training_time = stop - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esporto il modello su un file esterno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if to_export_model:\n",
    "    # Create the model filename\n",
    "    model_filename = \"\"\n",
    "    # Model type\n",
    "    model_filename += \"random_forest\"\n",
    "    # Enumerate model of the same type\n",
    "    model_filename += \"_\" + str(max(model_number, scaler_number))\n",
    "    # File extension\n",
    "    model_filename += \".sav\"\n",
    "    # Export the model\n",
    "    joblib.dump(random_forest_classifier, OUTPUT_PATH_MODEL + \"/\" + model_filename)\n",
    "    print(\"Modello esportato in:\", OUTPUT_PATH_MODEL + \"/\" + model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizzo il comportamento del modello sui dati di test che non ha mai visto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = random_forest_classifier.predict(X_test)\n",
    "score = accuracy_score(Y_test, Y_predicted)\n",
    "\n",
    "print(\"L'accuratezza è\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curva ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = random_forest_classifier.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(Y_test, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrice di confusione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_true=Y_test, y_pred=Y_predicted)\n",
    "conf_matrix = conf_matrix / conf_matrix.astype(np.float64).sum(axis=1)\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(conf_matrix, annot=True, vmin=0.0, vmax=1.0, fmt=\".2f\", cmap=\"Blues\", ax=ax)\n",
    "\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(['Active', 'Bankruptcy'])\n",
    "ax.yaxis.set_ticklabels(['Active', 'Bankruptcy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostro l'importanza di ogni feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,50))\n",
    "feat_importances = pd.Series(random_forest_classifier.feature_importances_, index=X_features_names).sort_values(ascending=False)\n",
    "feat_importances.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisi geografica (se possibile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if region_names:\n",
    "    region_accuracies = []\n",
    "    region_specificities = []\n",
    "    region_precisions = []\n",
    "    region_recalls = []\n",
    "    region_f1s = []\n",
    "    region_aucs = []\n",
    "\n",
    "    # For each region compute classification metrics\n",
    "    for region_name in region_names[:20]:\n",
    "        region_Y_test = Y_test[X_test[region_name] == 1]\n",
    "        region_Y_predicted = random_forest_classifier.predict(X_test[X_test[region_name] == 1])\n",
    "\n",
    "        region_accuracies.append(accuracy_score(region_Y_test, region_Y_predicted))\n",
    "        region_specificities.append(recall_score(region_Y_test, region_Y_predicted, pos_label=0))\n",
    "        region_precisions.append(precision_score(region_Y_test, region_Y_predicted))\n",
    "        region_recalls.append(recall_score(region_Y_test, region_Y_predicted))\n",
    "        region_f1s.append(f1_score(region_Y_test, region_Y_predicted))\n",
    "\n",
    "        region_probs = random_forest_classifier.predict_proba(X_test[X_test[region_name] == 1])\n",
    "        region_preds = region_probs[:,1]\n",
    "        region_fpr, region_tpr, region_threshold = metrics.roc_curve(region_Y_test, region_preds)\n",
    "        region_aucs.append(metrics.auc(region_fpr, region_tpr))\n",
    "\n",
    "\n",
    "    # Aggregate results into a single dataframe\n",
    "\n",
    "    region_metrics = pd.DataFrame()\n",
    "\n",
    "    region_metrics[\"Accuracy\"] = region_accuracies\n",
    "    region_metrics[\"Specificity\"] = region_specificities\n",
    "    region_metrics[\"Precision\"] = region_precisions\n",
    "    region_metrics[\"Recall\"] = region_recalls\n",
    "    region_metrics[\"F1-score\"] = region_f1s\n",
    "    region_metrics[\"AUC\"] = region_aucs\n",
    "\n",
    "    region_metrics.index = region_names[:20]\n",
    "    region_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if to_export_region_metrics and region_names:\n",
    "    export_name = \"RF_\"\n",
    "    if avoid_imbalanced_training:\n",
    "        export_name += (imbalanced_data_technique + \"_\")\n",
    "    else:\n",
    "        export_name += (\"Unbalanced\" + \"_\")\n",
    "    export_name += PATH_DATASET.split(\"\\\\\")[-1]\n",
    "\n",
    "    print(\"Metriche di classificazione esportate in\", OUTPUT_REGION_PATH + \"/\" + export_name + \".xlsx\")\n",
    "    region_metrics.to_excel(OUTPUT_REGION_PATH + \"/\" + export_name + \".xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stampo gli SHAP value inerenti al modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_plot:\n",
    "    explainer = shap.TreeExplainer(random_forest_classifier)\n",
    "    shap_values = explainer.shap_values(X_test, approximate=True)\n",
    "    shap.summary_plot(shap_values, X_test, feature_names=X_features_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prelevo i dati non utilizzati per testare il modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the complete dataset PATH\n",
    "path_complete_dataset = re.sub(\"filtered\", \"complete\", PATH_DATASET)\n",
    "path_complete_dataset = re.sub(\"_[0-9]{1,2}\\.pkl$\", \".pkl\", path_complete_dataset)\n",
    "path_complete_dataset = re.sub(\"_[0-9]{1,2}_region\\.pkl$\", \"_region.pkl\", path_complete_dataset)\n",
    "\n",
    "# Read the complete dataset\n",
    "complete_dataset = pd.read_pickle(path_complete_dataset)\n",
    "print(\"Lettura dataset:\", path_complete_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only the record not used during the previous phase\n",
    "validation_dataset = complete_dataset[~(complete_dataset['Ragione sociale'].isin(dataset['Ragione sociale'])\n",
    "                                        & complete_dataset['Province'].isin(dataset['Province'])\n",
    "                                        & complete_dataset['Accounting closing date'].isin(dataset['Accounting closing date'])\n",
    "                                        )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X validation dataset\n",
    "X_validation = validation_dataset.copy()\n",
    "X_validation.drop(removed_columns, axis=1, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Y validation dataset\n",
    "Y_validation = validation_dataset['Legal Status'].copy()\n",
    "Y_validation.replace({\"Active\": 0, \"Bankruptcy\": 1}, inplace=True)\n",
    "\n",
    "# Manage additional features\n",
    "if additional_features:\n",
    "    # One hot encoding\n",
    "\n",
    "    if \"Legal Form\" in X_validation.columns:\n",
    "        X_validation = X_validation.join(pd.get_dummies(validation_dataset['Legal Form']))\n",
    "        X_validation.drop('Legal Form', axis=1, inplace=True)\n",
    "\n",
    "    if \"Company Size\" in X_validation.columns:\n",
    "        X_validation = X_validation.join(pd.get_dummies(validation_dataset['Company Size']))\n",
    "        X_validation.drop('Company Size', axis=1, inplace=True)\n",
    "\n",
    "    if \"Regione\" in X_validation.columns:\n",
    "        X_validation = X_validation.join(pd.get_dummies(validation_dataset['Regione']))\n",
    "        X_validation.drop('Regione', axis=1, inplace=True)\n",
    "\n",
    "# Apply dimensionality reduction method\n",
    "if dimensionality_reduction:\n",
    "    X_validation = dimensionality_reduction_model.transform(X_validation)\n",
    "\n",
    "# Scale the X validation dataset\n",
    "if to_standardize:\n",
    "    # Standardize only the features without that are not one hot encoded\n",
    "    X_validation = pd.DataFrame(scaler.transform(X_validation))\n",
    "    X_validation.columns = X_features_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribuzione dei dati non visti dal modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_validation.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test del modello sui dati del dataset completo non utilizzati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_validation_predicted = random_forest_classifier.predict(X_validation)\n",
    "score = accuracy_score(Y_validation, Y_validation_predicted)\n",
    "\n",
    "print(\"L'accuratezza è\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = random_forest_classifier.predict_proba(X_validation)\n",
    "Y_validation_preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(Y_validation, Y_validation_preds)\n",
    "roc_auc_validation = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc_validation)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_true=Y_validation, y_pred=Y_validation_predicted)\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt='g', ax=ax)\n",
    "\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(['Active', 'Bankruptcy'])\n",
    "ax.yaxis.set_ticklabels(['Active', 'Bankruptcy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controllo se esiste il dataset di output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only if we want to export\n",
    "if to_export:\n",
    "    if exists(OUTPUT_PATH + \"/ML_model_experiments.pkl\"):\n",
    "        # If the dataset exists, read it\n",
    "        output_dataset = pd.read_pickle(OUTPUT_PATH + \"/ML_model_experiments.pkl\")\n",
    "        print(\"Dataset di output trovato\")\n",
    "    else:\n",
    "        # Otherwise create it\n",
    "        output_dataset = pd.DataFrame()\n",
    "        print(\"Dataset di output non trovato, creo un nuovo dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esporto l'esperimento nel dataset di output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if to_export:\n",
    "    if not avoid_imbalanced_training:\n",
    "        imbalanced_data_technique = \"N.A.\"\n",
    "\n",
    "    if not dimensionality_reduction:\n",
    "        dimensionality_reduction_method = \"N.A.\"\n",
    "\n",
    "    # Compute metrics to be added to the dataset\n",
    "    accuracy = accuracy_score(Y_test, Y_predicted)\n",
    "    specificity = recall_score(Y_test, Y_predicted, pos_label=0)\n",
    "    precision = precision_score(Y_test, Y_predicted)\n",
    "    recall = recall_score(Y_test, Y_predicted)\n",
    "    f1 = f1_score(Y_test, Y_predicted)\n",
    "    mcc = matthews_corrcoef(Y_test, Y_predicted)\n",
    "\n",
    "    accuracy_validation = accuracy_score(Y_validation, Y_validation_predicted)\n",
    "    specificity_validation = recall_score(Y_validation, Y_validation_predicted, pos_label=0)\n",
    "    precision_validation = precision_score(Y_validation, Y_validation_predicted)\n",
    "    recall_validation = recall_score(Y_validation, Y_validation_predicted)\n",
    "    f1_validation = f1_score(Y_validation, Y_validation_predicted)\n",
    "    mcc_validation = matthews_corrcoef(Y_validation, Y_validation_predicted)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(Y_test, Y_predicted).ravel()\n",
    "    tn_validation, fp_validation, fn_validation, tp_validation = confusion_matrix(Y_validation, Y_validation_predicted).ravel()\n",
    "\n",
    "\n",
    "    # Create a new record\n",
    "    new_record = pd.DataFrame({\"Date\": [date.today()],\n",
    "                               \"Model type\": [\"Random Forest\"],\n",
    "                               \"Model parameters\": [random_forest_classifier.get_params()],\n",
    "                               \"Dataset - train/test\": [PATH_DATASET.split(\"\\\\\")[-1]],\n",
    "                               \"Features\": [X_features_names],\n",
    "                               \"Standardized\": [to_standardize],\n",
    "                               \"Dimensionality reduction\": [dimensionality_reduction],\n",
    "                               \"Dimensionality reduction technique\": [dimensionality_reduction_method],\n",
    "                               \"Number of features\": [len(X_features_names)],\n",
    "                               \"Features importance\": [feat_importances.to_dict()],\n",
    "                               \"Imbalanced data corrections\": [avoid_imbalanced_training],\n",
    "                               \"Imbalanced data technique\": [imbalanced_data_technique],\n",
    "                               \"Number of active companies - train/test\": [Y_dataset.value_counts()[0]],\n",
    "                               \"Number of bankruptcy companies - train/test\": [Y_dataset.value_counts()[1]],\n",
    "                               \"Train/Test split\": [train_test_split_amount],\n",
    "                               \"Training time\": [training_time],\n",
    "                               \"Random state\": [rnd_state],\n",
    "                               \"TN - test\": [tn],\n",
    "                               \"TP - test\": [tp],\n",
    "                               \"FN - test\": [fn],\n",
    "                               \"FP - test\": [fp],\n",
    "                               \"Accuracy - test\": [accuracy],\n",
    "                               \"Specificity - test\": [specificity],\n",
    "                               \"Precision - test\": [precision],\n",
    "                               \"Recall - test\": [recall],\n",
    "                               \"F1-score - test\": [f1],\n",
    "                               \"MCC - test\": [mcc],\n",
    "                               \"AUC - test\": [roc_auc],\n",
    "                               \"Validation dataset\": [path_complete_dataset.split(\"\\\\\")[-1]],\n",
    "                               \"Number of active companies - validation\": Y_validation.value_counts()[0],\n",
    "                               \"Number of bankruptcy companies - validation\": [Y_validation.value_counts()[1]],\n",
    "                               \"TN - validation\": [tn_validation],\n",
    "                               \"TP - validation\": [tp_validation],\n",
    "                               \"FN - validation\": [fn_validation],\n",
    "                               \"FP - validation\": [fp_validation],\n",
    "                               \"Accuracy - validation\": [accuracy_validation],\n",
    "                               \"Specificity - validation\": [specificity_validation],\n",
    "                               \"Precision - validation\": [precision_validation],\n",
    "                               \"Recall - validation\": [recall_validation],\n",
    "                               \"F1-score - validation\": [f1_validation],\n",
    "                               \"MCC - validation\": [mcc_validation],\n",
    "                               \"AUC - validation\": [roc_auc_validation]})\n",
    "\n",
    "    # Append the new record\n",
    "    output_dataset = pd.concat([output_dataset, new_record], ignore_index=True, axis=0)\n",
    "\n",
    "    # Export the dataset\n",
    "    output_dataset.to_pickle(OUTPUT_PATH + \"/ML_model_experiments.pkl\")\n",
    "    print(\"Esprimento aggiunto al dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provo a migliorare l'accuratezza del modello con la tecnica del cross-validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_split = 5\n",
    "# prepare the cross-validation procedure\n",
    "cv = KFold(n_splits=num_split, random_state=rnd_state, shuffle=True)\n",
    "# create model\n",
    "random_forest_classifier_cv = RandomForestClassifier(random_state=rnd_state, n_jobs=n_jobs)\n",
    "# evaluate model\n",
    "scores = cross_val_score(random_forest_classifier_cv, X_dataset, Y_dataset, scoring='f1', cv=cv, n_jobs=n_jobs)\n",
    "# report performance\n",
    "print(\"L'accuratezza con\", num_split, \"split è\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testiamo diverse random forest con parametri diversi (hypertuning parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each list contains all the value of a specific hyperparameter we want to test\n",
    "random_forest_hyp_n_estimators = [50, 100, 150, 200]\n",
    "random_forest_hyp_max_features = [\"auto\", \"sqrt\", \"log2\"]\n",
    "random_forest_hyp_criterions = [\"entropy\", \"gini\"]\n",
    "\n",
    "# Create dictionary with each desired hyperparameter\n",
    "random_forest_hyp_grid = {'n_estimators': random_forest_hyp_n_estimators,\n",
    "                          'max_features': random_forest_hyp_max_features,\n",
    "                          'criterion': random_forest_hyp_criterions}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "rf_grid = GridSearchCV(estimator=RandomForestClassifier(),\n",
    "                       param_grid=random_forest_hyp_grid,\n",
    "                       cv=2,\n",
    "                       scoring=\"f1\",\n",
    "                       n_jobs=4,\n",
    "                       pre_dispatch=\"2*n_jobs\",\n",
    "                       verbose=4)\n",
    "# Fit the random search model\n",
    "rf_grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stampo i parametri con lo score migliore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Parameters:\", rf_grid.best_params_)\n",
    "print(\"Accuracy:\", rf_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dataset = pd.DataFrame(rf_grid.cv_results_).sort_values(by=['mean_test_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dataset.to_excel(OUTPUT_PATH + \"/grid_search.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
