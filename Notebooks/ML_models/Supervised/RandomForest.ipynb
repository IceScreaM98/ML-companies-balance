{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Secondo prototipo di modello ML: random forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Liberie varie da installare"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install sklearn\n",
    "#!pip install seaborn\n",
    "#!pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inclusione delle librerie utilizzate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "from datetime import date\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, SparsePCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Variabili di gestione files, parametri del modello e della fase di training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "######################################## INPUT DATASET ###########################################\n",
    "# Path of the dataset in .pkl format, can be changed\n",
    "PATH_DATASET = r\"C:\\Users\\Andre\\OneDrive - Università degli Studi di Parma\\Tirocinio\\Dataset_output\\filtered_active_bankruptcy_big_history_12.pkl\"\n",
    "##################################################################################################\n",
    "\n",
    "######################################## DATA STANDARDIZATION ####################################\n",
    "# True = Standardize data, can be changed\n",
    "to_standardize = True\n",
    "##################################################################################################\n",
    "\n",
    "######################################## DATA CLASSES BALANCE ####################################\n",
    "# True = Oversample the least populated class (Bankruptcy), can be changed\n",
    "avoid_imbalanced_training = True\n",
    "\n",
    "# Oversample/Undersample the least/most populated class (Bankruptcy), possible values: 'Oversample'\n",
    "# or 'Undersample'\n",
    "# It only affects the notebook if avoid_imbalanced_training is True\n",
    "imbalanced_data_technique = \"Oversample\"\n",
    "##################################################################################################\n",
    "\n",
    "######################################## DIMENSIONALITY REDUCTION ################################\n",
    "# True = use a dimensionality reduction method, can be changed\n",
    "dimensionality_reduction = False\n",
    "\n",
    "# Applies dimensionality reduction method, possible values: 'PCA', 'LDA', 'Incremental PCA',\n",
    "# 'Sparse PCA'\n",
    "# It only affects the notebook if dimensionality_reduction is True\n",
    "dimensionality_reduction_method = \"PCA\"\n",
    "\n",
    "# Number of desired components after applying a dimensionality reduction method\n",
    "# N.B. n_components must be < number of input features and LDA only supports 1 components in\n",
    "# our case\n",
    "n_components = 25\n",
    "##################################################################################################\n",
    "\n",
    "######################################## EXTRA DATASET PARAMETERS ################################\n",
    "# True = Also use non financial indexes features like the legal form or the size of the company\n",
    "additional_features = False\n",
    "##################################################################################################\n",
    "\n",
    "######################################## TRAIN-TEST SPLIT ########################################\n",
    "# A value between [0, 1], it represent the percentage of records not used during training time,\n",
    "# can be changed\n",
    "train_test_split_amount = 0.25\n",
    "##################################################################################################\n",
    "\n",
    "######################################## EXTRA MODEL PARAMETERS ##################################\n",
    "# Select a random state value in order to control the randomness effect, can be changed\n",
    "rnd_state = 25\n",
    "\n",
    "# Specify the number of cuncurrent jobs in order to speed up certain traning phases.\n",
    "# Specify -1 in order to use all the job available, the default one is 1, can be changed\n",
    "n_jobs = -1\n",
    "##################################################################################################\n",
    "\n",
    "######################################## OUTPUT EXPERIMENT #######################################\n",
    "# True = Export the current experiment inside the dataframe that collects all of them, can be changed\n",
    "to_export = True\n",
    "\n",
    "# Path of the dataset in .pkl format to store all the experiments, can be changed and\n",
    "# if not present it will be created\n",
    "OUTPUT_PATH = r\"C:\\Users\\Andre\\OneDrive - Università degli Studi di Parma\\Tirocinio\\ML_models\"\n",
    "##################################################################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lettura del dataset di input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(PATH_DATASET)\n",
    "print(\"Il dataset da utilizzare ha\", dataset.shape[0], \"record e\", dataset.shape[1], \"colonne\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suddivisione del dataset in X e Y, dove X sono le features in ingresso e Y è la risposta in output (attivo/bancarotta)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove descriptive columns\n",
    "removed_columns = [\n",
    "    \"Ragione sociale\",\n",
    "    \"Province\",\n",
    "    \"Accounting closing date\",\n",
    "    \"Legal Status\",\n",
    "    \"Tax code number\",\n",
    "    \"CCIAA number\"\n",
    "]\n",
    "\n",
    "if not additional_features:\n",
    "    removed_columns.append(\"Legal Form\")\n",
    "    removed_columns.append(\"Company Size\")\n",
    "    removed_columns.append(\"Number of employees\")\n",
    "\n",
    "# X dataset\n",
    "X_dataset = dataset.copy()\n",
    "X_dataset.drop(removed_columns, axis=1, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Manage additional features\n",
    "if additional_features:\n",
    "    # One hot encoding\n",
    "    X_dataset = X_dataset.join(pd.get_dummies(dataset['Legal Form']))\n",
    "    X_dataset.drop('Legal Form', axis=1, inplace=True)\n",
    "\n",
    "    X_dataset = X_dataset.join(pd.get_dummies(dataset['Company Size']))\n",
    "    X_dataset.drop('Company Size', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Save the new feature names\n",
    "X_features_names = X_dataset.columns.to_list()\n",
    "\n",
    "# Y dataset\n",
    "Y_dataset = dataset['Legal Status'].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stampo i primi record dei 2 nuovi dataset per chiarezza"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_dataset.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_dataset.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Codifico la variabile di risposta (Active/Bankruptcy) in (0/1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_dataset.replace({\"Active\": 0, \"Bankruptcy\": 1}, inplace=True)\n",
    "Y_dataset.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Controllo il numero di record per ciascuna classe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_dataset.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Uso la tecnica di random oversampling o undersampling per evitare un allenamento di un modello con classi sbilanciate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if avoid_imbalanced_training:\n",
    "    # Oversample (provare anche per undersampling)\n",
    "    if imbalanced_data_technique == \"Oversample\":\n",
    "        sm = SMOTE(random_state=rnd_state, n_jobs=n_jobs)\n",
    "        X_dataset, Y_dataset = sm.fit_resample(X_dataset, Y_dataset)\n",
    "    # Undersample\n",
    "    elif imbalanced_data_technique == \"Undersample\":\n",
    "        undersample = RandomUnderSampler(sampling_strategy='majority', random_state=rnd_state)\n",
    "        X_dataset, Y_dataset = undersample.fit_resample(X_dataset, Y_dataset)\n",
    "    else:\n",
    "        print(\"Error: wrong variable value about imbalanced data\")\n",
    "Y_dataset.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Applico algoritmo di riduzione della dimensionalità"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if dimensionality_reduction:\n",
    "    if dimensionality_reduction_method == \"PCA\":\n",
    "        dimensionality_reduction_model = PCA(n_components=n_components)\n",
    "        X_dataset = dimensionality_reduction_model.fit_transform(X_dataset)\n",
    "    elif dimensionality_reduction_method == \"Incremental PCA\":\n",
    "        dimensionality_reduction_model = IncrementalPCA(n_components=n_components)\n",
    "        X_dataset = dimensionality_reduction_model.fit_transform(X_dataset)\n",
    "    elif dimensionality_reduction_method == \"Sparse PCA\":\n",
    "        dimensionality_reduction_model = SparsePCA(n_components=n_components)\n",
    "        X_dataset = dimensionality_reduction_model.fit_transform(X_dataset)\n",
    "    elif dimensionality_reduction_method == \"LDA\":\n",
    "        n_components = 1\n",
    "        dimensionality_reduction_model = LinearDiscriminantAnalysis(n_components=n_components)\n",
    "        X_dataset = dimensionality_reduction_model.fit_transform(X_dataset, Y_dataset)\n",
    "    # Need to change feature names, because they're lost in the new dataset\n",
    "    X_features_names = [\"Component \" + str(i) for i in range(0, n_components)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Standardizzo i dati contenuti in X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if to_standardize:\n",
    "    scaler = StandardScaler()\n",
    "    X_dataset = scaler.fit_transform(X_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Divido i 2 dataset in train e test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_dataset,\n",
    "                                                    Y_dataset,\n",
    "                                                    stratify=Y_dataset,\n",
    "                                                    test_size=train_test_split_amount,\n",
    "                                                    random_state=rnd_state)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creo primo prototipo di random forest e lo alleno su una porzione dei dati"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random_forest_classifier = RandomForestClassifier(random_state=rnd_state, n_jobs=n_jobs)\n",
    "start = time.time()\n",
    "random_forest_classifier.fit(X_train, Y_train)\n",
    "stop = time.time()\n",
    "training_time = stop - start"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analizzo il comportamento del modello sui dati di test che non ha mai visto"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_predicted = random_forest_classifier.predict(X_test)\n",
    "score = accuracy_score(Y_test, Y_predicted)\n",
    "\n",
    "print(\"L'accuratezza è\", score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Curva ROC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probs = random_forest_classifier.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(Y_test, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Matrice di confusione"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_true=Y_test, y_pred=Y_predicted)\n",
    "conf_matrix = conf_matrix / conf_matrix.astype(np.float64).sum(axis=1)\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(conf_matrix, annot=True, vmin=0.0, vmax=1.0, fmt=\".2f\", cmap=\"Blues\", ax=ax)\n",
    "\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(['Active', 'Bankruptcy'])\n",
    "ax.yaxis.set_ticklabels(['Active', 'Bankruptcy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mostro l'importanza di ogni feature"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,50))\n",
    "feat_importances = pd.Series(random_forest_classifier.feature_importances_, index=X_features_names).sort_values(ascending=False)\n",
    "feat_importances.plot(kind='barh')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prelevo i dati non utilizzati per testare il modello"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Take the complete dataset PATH\n",
    "path_complete_dataset = re.sub(\"filtered\", \"complete\", PATH_DATASET)\n",
    "path_complete_dataset = re.sub(\"_[0-9]{1,2}\\.pkl$\", \".pkl\", path_complete_dataset)\n",
    "\n",
    "# Read the complete dataset\n",
    "complete_dataset = pd.read_pickle(path_complete_dataset)\n",
    "print(\"Lettura dataset:\", path_complete_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Take only the record not used during the previous phase\n",
    "validation_dataset = complete_dataset[~complete_dataset.index.isin(dataset.index)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# X validation dataset\n",
    "X_validation = validation_dataset.copy()\n",
    "X_validation.drop(removed_columns, axis=1, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Y validation dataset\n",
    "Y_validation = validation_dataset['Legal Status'].copy()\n",
    "Y_validation.replace({\"Active\": 0, \"Bankruptcy\": 1}, inplace=True)\n",
    "\n",
    "# Manage additional features\n",
    "if additional_features:\n",
    "    # One hot encoding\n",
    "    X_validation = X_validation.join(pd.get_dummies(validation_dataset['Legal Form']))\n",
    "    X_validation.drop('Legal Form', axis=1, inplace=True)\n",
    "\n",
    "    X_validation = X_validation.join(pd.get_dummies(validation_dataset['Company Size']))\n",
    "    X_validation.drop('Company Size', axis=1, inplace=True)\n",
    "\n",
    "# Apply dimensionality reduction method\n",
    "if dimensionality_reduction:\n",
    "    X_validation = dimensionality_reduction_model.transform(X_validation)\n",
    "\n",
    "# Scale the X validation dataset\n",
    "if to_standardize:\n",
    "    X_validation = scaler.transform(X_validation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Distribuzione dei dati non visti dal modello"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_validation.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test del modello sui dati del dataset completo non utilizzati"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_validation_predicted = random_forest_classifier.predict(X_validation)\n",
    "score = accuracy_score(Y_validation, Y_validation_predicted)\n",
    "\n",
    "print(\"L'accuratezza è\", score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probs = random_forest_classifier.predict_proba(X_validation)\n",
    "Y_validation_preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(Y_validation, Y_validation_preds)\n",
    "roc_auc_validation = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc_validation)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_true=Y_validation, y_pred=Y_validation_predicted)\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt='g', ax=ax)\n",
    "\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(['Active', 'Bankruptcy'])\n",
    "ax.yaxis.set_ticklabels(['Active', 'Bankruptcy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Controllo se esiste il dataset di output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Only if we want to export\n",
    "if to_export:\n",
    "    if exists(OUTPUT_PATH + \"/ML_model_experiments.pkl\"):\n",
    "        # If the dataset exists, read it\n",
    "        output_dataset = pd.read_pickle(OUTPUT_PATH + \"/ML_model_experiments.pkl\")\n",
    "        print(\"Dataset di output trovato\")\n",
    "    else:\n",
    "        # Otherwise create it\n",
    "        output_dataset = pd.DataFrame()\n",
    "        print(\"Dataset di output non trovato, creo un nuovo dataset\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Esporto l'esperimento nel dataset di output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if to_export:\n",
    "    if not avoid_imbalanced_training:\n",
    "        imbalanced_data_technique = \"N.A.\"\n",
    "\n",
    "    if not dimensionality_reduction:\n",
    "        dimensionality_reduction_method = \"N.A.\"\n",
    "\n",
    "    # Compute metrics to be added to the dataset\n",
    "    accuracy = accuracy_score(Y_test, Y_predicted)\n",
    "    specificity = recall_score(Y_test, Y_predicted, pos_label=0)\n",
    "    precision = precision_score(Y_test, Y_predicted)\n",
    "    recall = recall_score(Y_test, Y_predicted)\n",
    "    f1 = f1_score(Y_test, Y_predicted)\n",
    "\n",
    "    accuracy_validation = accuracy_score(Y_validation, Y_validation_predicted)\n",
    "    specificity_validation = recall_score(Y_validation, Y_validation_predicted, pos_label=0)\n",
    "    precision_validation = precision_score(Y_validation, Y_validation_predicted)\n",
    "    recall_validation = recall_score(Y_validation, Y_validation_predicted)\n",
    "    f1_validation = f1_score(Y_validation, Y_validation_predicted)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(Y_test, Y_predicted).ravel()\n",
    "    tn_validation, fp_validation, fn_validation, tp_validation = confusion_matrix(Y_validation, Y_validation_predicted).ravel()\n",
    "\n",
    "\n",
    "    # Create a new record\n",
    "    new_record = pd.DataFrame({\"Date\": [date.today()],\n",
    "                               \"Model type\": [\"Random Forest\"],\n",
    "                               \"Model parameters\": [random_forest_classifier.get_params()],\n",
    "                               \"Dataset - train/test\": [PATH_DATASET.split(\"\\\\\")[-1]],\n",
    "                               \"Features\": [X_features_names],\n",
    "                               \"Standardized\": [to_standardize],\n",
    "                               \"Dimensionality reduction\": [dimensionality_reduction],\n",
    "                               \"Dimensionality reduction technique\": [dimensionality_reduction_method],\n",
    "                               \"Number of features\": [len(X_features_names)],\n",
    "                               \"Features importance\": [feat_importances.to_dict()],\n",
    "                               \"Imbalanced data corrections\": [avoid_imbalanced_training],\n",
    "                               \"Imbalanced data technique\": [imbalanced_data_technique],\n",
    "                               \"Number of active companies - train/test\": [Y_dataset.value_counts()[0]],\n",
    "                               \"Number of bankruptcy companies - train/test\": [Y_dataset.value_counts()[1]],\n",
    "                               \"Train/Test split\": [train_test_split_amount],\n",
    "                               \"Training time\": [training_time],\n",
    "                               \"Random state\": [rnd_state],\n",
    "                               \"TN - test\": [tn],\n",
    "                               \"TP - test\": [tp],\n",
    "                               \"FN - test\": [fn],\n",
    "                               \"FP - test\": [fp],\n",
    "                               \"Accuracy - test\": [accuracy],\n",
    "                               \"Specificity - test\": [specificity],\n",
    "                               \"Precision - test\": [precision],\n",
    "                               \"Recall - test\": [recall],\n",
    "                               \"F1-score - test\": [f1],\n",
    "                               \"AUC - test\": [roc_auc],\n",
    "                               \"Validation dataset\": [path_complete_dataset.split(\"\\\\\")[-1]],\n",
    "                               \"Number of active companies - validation\": Y_validation.value_counts()[0],\n",
    "                               \"Number of bankruptcy companies - validation\": [Y_validation.value_counts()[1]],\n",
    "                               \"TN - validation\": [tn_validation],\n",
    "                               \"TP - validation\": [tp_validation],\n",
    "                               \"FN - validation\": [fn_validation],\n",
    "                               \"FP - validation\": [fp_validation],\n",
    "                               \"Accuracy - validation\": [accuracy_validation],\n",
    "                               \"Specificity - validation\": [specificity_validation],\n",
    "                               \"Precision - validation\": [precision_validation],\n",
    "                               \"Recall - validation\": [recall_validation],\n",
    "                               \"F1-score - validation\": [f1_validation],\n",
    "                               \"AUC - validation\": [roc_auc_validation]})\n",
    "\n",
    "    # Append the new record\n",
    "    output_dataset = pd.concat([output_dataset, new_record], ignore_index=True, axis=0)\n",
    "\n",
    "    # Export the dataset\n",
    "    output_dataset.to_pickle(OUTPUT_PATH + \"/ML_model_experiments.pkl\")\n",
    "    print(\"Esprimento aggiunto al dataset\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Provo a migliorare l'accuratezza del modello con la tecnica del cross-validation score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_split = 5\n",
    "# prepare the cross-validation procedure\n",
    "cv = KFold(n_splits=num_split, random_state=rnd_state, shuffle=True)\n",
    "# create model\n",
    "random_forest_classifier_cv = RandomForestClassifier(random_state=rnd_state, n_jobs=n_jobs)\n",
    "# evaluate model\n",
    "scores = cross_val_score(random_forest_classifier_cv, X_dataset, Y_dataset, scoring='f1', cv=cv, n_jobs=n_jobs)\n",
    "# report performance\n",
    "print(\"L'accuratezza con\", num_split, \"split è\", np.mean(scores))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testiamo diverse random forest con parametri diversi (hypertuning parameters)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Each list contains all the value of a specific hyperparameter we want to test\n",
    "random_forest_hyp_n_estimators = [100, 150, 200]\n",
    "random_forest_hyp_max_features = [\"auto\", \"sqrt\", \"log2\"]\n",
    "random_forest_hyp_max_depth = [3, 5, 7, 9, 11, 13, 15, None]\n",
    "random_forest_hyp_criterions = [\"entropy\", \"gini\"]\n",
    "random_forest_hyp_bootstrap = [True, False]\n",
    "\n",
    "# Create dictionary with each desired hyperparameter\n",
    "random_forest_hyp_grid = {'n_estimators': random_forest_hyp_n_estimators,\n",
    "                          'max_features': random_forest_hyp_max_features,\n",
    "                          'max_depth': random_forest_hyp_max_depth,\n",
    "                          'criterion': random_forest_hyp_criterions,\n",
    "                          'bootstrap': random_forest_hyp_bootstrap}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "random_forest_classifier_grid = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = random_forest_classifier_grid,\n",
    "                               param_distributions = random_forest_hyp_grid,\n",
    "                               n_iter=50,\n",
    "                               cv=2,\n",
    "                               scoring=\"accuracy\",\n",
    "                               random_state=rnd_state,\n",
    "                               n_jobs=6)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, Y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stampo i parametri con lo score migliore"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Parameters:\", rf_random.best_params_)\n",
    "print(\"Accuracy:\", rf_random.best_score_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}