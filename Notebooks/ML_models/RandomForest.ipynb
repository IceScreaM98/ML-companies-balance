{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Secondo prototipo di modello ML: random forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Liberie varie da installare"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install sklearn\n",
    "#!pip install seaborn\n",
    "#!pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inclusione delle librerie utilizzate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Variabili di gestione files, parametri del modello e della fase di training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Path of the dataset in .pkl format, can be changed\n",
    "PATH_DATASET = r\"C:\\Users\\Andre\\OneDrive - Università degli Studi di Parma\\Tirocinio\\Dataset_output\\filtered_active_bankruptcy_big.pkl\"\n",
    "\n",
    "# True = Standardize data, can be changed\n",
    "to_standardize = True\n",
    "\n",
    "# True = Oversample/Undersample the least/most populated class (Bankruptcy), can be changed\n",
    "avoid_imbalanced_training = True\n",
    "\n",
    "# Oversample or Undersample, can be changed.\n",
    "# It only affects the notebook if avoid_imbalanced_training is True\n",
    "imbalanced_data_technique = \"Undersample\"\n",
    "\n",
    "# True = Also use non financial indexes features like the legal form or the size of the company\n",
    "additional_features = True\n",
    "\n",
    "# A value between [0, 1], it represent the percentage of records not used during training time, can be changed\n",
    "train_test_split_amount = 0.25\n",
    "\n",
    "# Select a random state value in order to control the randomness effect, can be changed\n",
    "rnd_state = 25\n",
    "\n",
    "# Specify the number of cuncurrent jobs in order to speed up certain traning phases.\n",
    "# Specify -1 in order to use all the job available, the default one is 1, can be changed\n",
    "n_jobs = 6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lettura del dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(PATH_DATASET)\n",
    "print(\"Il dataset da utilizzare ha\", dataset.shape[0], \"record e\", dataset.shape[1], \"colonne\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suddivisione del dataset in X e Y, dove X sono le features in ingresso (indicatori finanziari) e Y è la risposta in output (attivo/bancarotta)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_features_names = ['PN/Totale Debiti',\n",
    "                    'Deb. Prev + Trib/Attivo',\n",
    "                    'Tempo medio riscossione (TMR)',\n",
    "                    'Tempo medio di pagamento (TMP)',\n",
    "                    'PFN/EBITDA',\n",
    "                    'PFN/PN',\n",
    "                    'Gearing',\n",
    "                    'ROS',\n",
    "                    'Working capital/net sales',\n",
    "                    'Cash/Current Liabilities',\n",
    "                    'Accounts receivable/inventory',\n",
    "                    'EBIT/interest expenses',\n",
    "                    'Att.Br/Attivo',\n",
    "                    'Ricavi/Attivo',\n",
    "                    'EBITDA/Totale Debiti']\n",
    "\n",
    "if additional_features:\n",
    "    X_features_names.append('Legal Form')\n",
    "    #X_features_names.append('Company Size')\n",
    "\n",
    "Y_feature_name = 'Legal Status'\n",
    "\n",
    "X_dataset = dataset[X_features_names].copy()\n",
    "\n",
    "if additional_features:\n",
    "    # One hot encoding\n",
    "    X_dataset = X_dataset.join(pd.get_dummies(dataset['Legal Form']))\n",
    "    X_dataset.drop('Legal Form', axis = 1, inplace=True)\n",
    "\n",
    "    #X_dataset = X_dataset.join(pd.get_dummies(dataset['Company Size']))\n",
    "    #X_dataset.drop('Company Size', axis = 1, inplace=True)\n",
    "\n",
    "    # Save the new feature names\n",
    "    X_features_names = X_dataset.columns.to_list()\n",
    "\n",
    "Y_dataset = dataset[Y_feature_name].copy()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stampo i primi record dei 2 nuovi dataset per chiarezza"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_dataset.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_dataset.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Codifico la variabile di risposta (Active/Bankruptcy) in (0/1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_dataset.replace({\"Active\": 0, \"Bankruptcy\": 1}, inplace=True)\n",
    "Y_dataset.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Controllo il numero di record per ciascuna classe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_dataset.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Uso la tecnica di random oversampling o undersampling per evitare un allenamento di un modello con classi sbilanciate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if avoid_imbalanced_training:\n",
    "    # Oversample\n",
    "    if imbalanced_data_technique == \"Oversample\":\n",
    "        sm = SMOTE(random_state=rnd_state, n_jobs=n_jobs)\n",
    "        X_dataset, Y_dataset = sm.fit_resample(X_dataset, Y_dataset)\n",
    "    # Undersample\n",
    "    elif imbalanced_data_technique == \"Undersample\":\n",
    "        undersample = RandomUnderSampler(sampling_strategy='majority', random_state=rnd_state)\n",
    "        X_dataset, Y_dataset = undersample.fit_resample(X_dataset, Y_dataset)\n",
    "    else:\n",
    "        print(\"Error: wrong variable value about imbalanced data\")\n",
    "Y_dataset.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Standardizzo i dati contenuti in X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if to_standardize:\n",
    "    scaler = StandardScaler()\n",
    "    X_dataset = scaler.fit_transform(X_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Divido i 2 dataset in train e test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_dataset,\n",
    "                                                    Y_dataset,\n",
    "                                                    stratify=Y_dataset,\n",
    "                                                    test_size=train_test_split_amount,\n",
    "                                                    random_state=rnd_state)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creo primo prototipo di random forest e lo alleno sui dati di train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random_forest_classifier = RandomForestClassifier(random_state=rnd_state, n_jobs=n_jobs)\n",
    "random_forest_classifier.fit(X_train, Y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Guardo come si comporta sui dati di test che il modello non ha mai visto"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_predicted = random_forest_classifier.predict(X_test)\n",
    "score = accuracy_score(Y_test, Y_predicted)\n",
    "\n",
    "print(\"L'accuratezza è\", score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Curva ROC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probs = random_forest_classifier.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(Y_test, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Matrice di confusione"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_true=Y_test, y_pred=Y_predicted)\n",
    "conf_matrix = conf_matrix / conf_matrix.astype(np.float64).sum(axis=1)\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(conf_matrix, annot=True, vmin=0.0, vmax=1.0, fmt=\".2f\", cmap=\"Blues\", ax=ax)\n",
    "\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(['Active', 'Bankruptcy'])\n",
    "ax.yaxis.set_ticklabels(['Active', 'Bankruptcy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mostro l'importanza di ogni feature"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "feat_importances = pd.Series(random_forest_classifier.feature_importances_, index=X_features_names)\n",
    "feat_importances.plot(kind='barh')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Provo utilizzando la tecnica del cross-validation score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_split = 5\n",
    "end_split = 21\n",
    "for n_split in range(start_split, end_split, 5):\n",
    "    # prepare the cross-validation procedure\n",
    "    cv = KFold(n_splits=n_split, random_state=rnd_state, shuffle=True)\n",
    "    # create model\n",
    "    random_forest_classifier_cv = RandomForestClassifier(random_state=rnd_state, n_jobs=n_jobs)\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(random_forest_classifier_cv, X_dataset, Y_dataset, scoring='accuracy', cv=cv, n_jobs=n_jobs)\n",
    "    # report performance\n",
    "    print(\"L'accuratezza con\", n_split, \"split è\", np.mean(scores))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testiamo diverse random forest con parametri diversi (hypertuning parameters)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Each list contains all the value of a specific parameter we want to test\n",
    "random_forest_parameter_criterions = [\"entropy\", \"gini\"]\n",
    "random_forest_parameter_min_samples_splits = [2, 5, 10, 100, 1000]\n",
    "random_forest_parameter_min_samples_leaves = [1, 2, 5, 10, 100, 1000]\n",
    "\n",
    "# List to save each score\n",
    "random_forest_scores = []\n",
    "\n",
    "# Iterate over each parameter\n",
    "for criterion in random_forest_parameter_criterions:\n",
    "    for min_samples_split in random_forest_parameter_min_samples_splits:\n",
    "        for min_samples_leaf in random_forest_parameter_min_samples_leaves:\n",
    "            # Create the decision tree with the desired parameters\n",
    "            test_random_forest = RandomForestClassifier(criterion=criterion,\n",
    "                                                        min_samples_split=min_samples_split,\n",
    "                                                        min_samples_leaf=min_samples_leaf,\n",
    "                                                        random_state=rnd_state,\n",
    "                                                        n_jobs=n_jobs)\n",
    "            # Train the model\n",
    "            test_random_forest.fit(X_train, Y_train)\n",
    "            # Test the model\n",
    "            Y_predicted = test_random_forest.predict(X_test)\n",
    "            # Obtain the accuracy\n",
    "            score = accuracy_score(Y_test, Y_predicted)\n",
    "            # Add the results to the list\n",
    "            random_forest_scores.append((criterion, min_samples_split, min_samples_leaf, score))\n",
    "            # Print the result (unordered)\n",
    "            print(\"Criterion:\", criterion,\n",
    "                  \"min_samples_split:\", min_samples_split,\n",
    "                  \"min_samples_leaf:\", min_samples_leaf,\n",
    "                  \"--- SCORE:\", score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the result (ordered)\n",
    "random_forest_scores.sort(key=lambda x:x[-1], reverse=True)\n",
    "\n",
    "for criterion, min_samples_split, min_samples_leaf, score in random_forest_scores:\n",
    "    print(\"Criterion:\", criterion,\n",
    "          \"min_samples_split:\", min_samples_split,\n",
    "          \"min_samples_leaf:\", min_samples_leaf,\n",
    "          \"--- SCORE:\", score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}